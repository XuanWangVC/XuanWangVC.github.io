# üìù Publications 
**Equal contribution**$^\star$
**Corresponding author**$^\dagger$

## Conference papers
<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2022</div><img src='images/pub/fenerf.gif' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[FENeRF: Face Editing in Neural Radiance Fields](https://arxiv.org/pdf/2111.15490.pdf)

Jingxiang Sun, **Xuan Wang**$^\dagger$, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, Jue Wang

[**Project**](https://mrtornado24.github.io/FENeRF/) <strong><span class='show_paper_citations' data='3xd3EAAAAJ:LkGwnXOMwfcC'></span></strong>
- The first portrait image generator that is locally editable and strictly view-consistent.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2022</div><img src='images/pub/hdrnerf.gif' type="video/mp4`" alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[HDR-NeRF: High Dynamic Range Neural Radiance Fields](https://arxiv.org/pdf/2111.14451.pdf)

Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, **Xuan Wang**, Qing Wang

[**Project**](https://shsf0817.github.io/hdr-nerf/) <strong><span class='show_paper_citations' data='3xd3EAAAAJ:roLk4NBRz8UC'></span></strong>
- High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an HDR radiance field from a set of low dynamic range (LDR) views with different exposures.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2022</div><img src='images/pub/hanerf.gif' type="video/mp4`" alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Hallucinated Neural Radiance Fields in the Wild](https://arxiv.org/pdf/2111.15246.pdf)

Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Feng Ying, **Xuan Wang**, Jue Wang

[**Project**](https://rover-xingyu.github.io/Ha-NeRF/) <strong><span class='show_paper_citations' data='3xd3EAAAAJ:UebtZRa9Y70C'></span></strong>
- An appearance hallucination module to handle time-varying appearances and transfer them to novel views.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2022</div><img src='images/pub/deblurrnerf.gif' type="video/mp4`" alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Deblur-NeRF: Neural Radiance Fields from Blurry Images](https://arxiv.org/pdf/2111.14292.pdf)

Li Ma, Xiaoyu Li, Jing Liao, Qi Zhang, **Xuan Wang**, Jue Wang, Pedro V Sander

[**Project**](https://limacv.github.io/deblurnerf/) <strong><span class='show_paper_citations' data='3xd3EAAAAJ:Se3iqnhoufwC'></span></strong>
- The first method that can recover a sharp NeRF from blurry input.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">ICCV 2019</div><img src='images/pub/boosting.png' type="video/mp4`" alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[On Boosting Single-Frame 3D Human Pose Estimation via Monocular Videos](https://arxiv.org/pdf/2111.14292.pdf)

Zhi Li$^\star$, **Xuan Wang**$^\star$, Fei Wang, Peilin Jiang

<strong><span class='show_paper_citations' data='3xd3EAAAAJ:zYLM7Y9cAGgC'></span></strong>
- The method that exploits monocular videos to complement the training dataset for the singleimage 3D human pose estimation tasks.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">ECCV 2016</div><img src='images/pub/templatefree.gif' type="video/mp4`" alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Template-free 3D Reconstruction of Poorly-textured Nonrigid Surfaces](https://infoscience.epfl.ch/record/221131/files/WangSalzmannWangZhaoECCV16.pdf)

**Xuan Wang**, Mathieu Salzmann, Fei Wang, Jizhong Zhao

[**Project**](https://infoscience.epfl.ch/record/221131) <strong><span class='show_paper_citations' data='3xd3EAAAAJ:d1gkVwhDpl0C'></span></strong>
- A template-free approach to reconstructing a poorly-textured, deformable surface.
</div>
</div>

<!--
# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
-->
