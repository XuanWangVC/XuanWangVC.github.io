## Papers in 2023

<div class='paper-box'><div class='paper-box-image'><div class="badge">ICCV 2023</div><img src='images/pub/toontalker.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[ToonTalker: Cross-Domain Face Reenactment](https://arxiv.org/pdf/2308.12866.pdf)

Yuan Gong, Yong Zhang, Xiaodong Cun, Fei Yin, Yanbo Fan, **Xuan Wang**, Baoyuan Wu, Yujiu Yang

<strong><span class='show_paper_citations' data=''></span></strong>
- We propose a novel method for cross-domain reenactment without paired data.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">SIGGRAPH 2023 (Conf)</div><img src='images/pub/NOFA.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[NOFA: NeRF-based One-shot Facial Avatar Reconstruction](https://arxiv.org/pdf/2307.03441.pdf)

Wangbo Yu, Yanbo Fan$^\dagger$, Yong Zhang$^\dagger$, **Xuan Wang**$^\dagger$, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu

<strong><span class='show_paper_citations' data=''></span></strong>
- We propose a one-shot 3D facial avatar reconstruction framework, which only requires a single source image to reconstruct high-fidelity 3D facial avatar, by leveraging the rich generative prior of 3D GAN and developing an efficient encoder-decoder network.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023 (Highlight)</div><img src='images/pub/next3d.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars](https://arxiv.org/pdf/2211.11208.pdf)

Jingxiang Sun, **Xuan Wang**, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, Yebin Liu

[**Project**](https://mrtornado24.github.io/Next3D/) | [![](https://img.shields.io/github/stars/MrTornado24/Next3D?style=flat-square&label=GitHub%20Star)](https://github.com/MrTornado24/Next3D)
 <strong><span class='show_paper_citations' data=''></span></strong>
- We propose a 3D representation called Generative Texture-Rasterized Tri-planes that learns Generative Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/uv_volumes.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[UV Volumes for Real-time Rendering of Editable Free-view Human Performance](https://arxiv.org/pdf/2203.14402.pdf)

Yue Chen$^\star$, **Xuan Wang**$^\star$, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, Fei Wang

[**Project**](https://fanegg.github.io/UV-Volumes/) | [![](https://img.shields.io/github/stars/fanegg/UV-Volumes?style=flat-square&label=GitHub%20Star)](https://github.com/fanegg/UV-Volumes)
<strong><span class='show_paper_citations' data=''></span></strong>
- We propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in real-time.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/L2G.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[L2G-NeRF: Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields](https://arxiv.org/pdf/2211.11505.pdf)

Yue Chen$^\star$, Xingyu Chen$^\star$, **Xuan Wang**$^\dagger$, Qi Zhang, Yu Guo$^\dagger$, Ying Shan, Fei Wang

[**Project**](https://rover-xingyu.github.io/L2G-NeRF/) | [![](https://img.shields.io/github/stars/rover-xingyu/L2G-NeRF?style=flat-square&label=GitHub%20Star)](https://github.com/rover-xingyu/L2G-NeRF)
<strong><span class='show_paper_citations' data=''></span></strong>
- We propose L2G-NeRF, a Local-to-Global registration method for bundle-adjusting Neural Radiance Fields: first, a pixel-wise flexible alignment, followed by a framewise constrained parametric alignment.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/NBS.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors](https://arxiv.org/pdf/2211.15064.pdf)

Yunpeng Bai, Yanbo Fan, **Xuan Wang**, Yong Zhang, Jingxiang Sun, Chun Yuan, Ying Shan

[![](https://img.shields.io/github/stars/bbaaii/HFA-GP?style=flat-square&label=GitHub%20Star)](https://github.com/bbaaii/HFA-GP)
<strong><span class='show_paper_citations' data=''></span></strong>
- We propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/sadtalker.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://arxiv.org/pdf/2211.12194.pdf)

Wenxuan Zhang, Xiaodong Cun, **Xuan Wang**, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang

[**Project**](https://github.com/OpenTalker/SadTalker) | [![](https://img.shields.io/github/stars/OpenTalker/SadTalker?style=flat-square&label=GitHub%20Star)](https://github.com/OpenTalker/SadTalker) ðŸ”¥
<strong><span class='show_paper_citations' data=''></span></strong>
- We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/symmetry.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[3D GAN Inversion with Facial Symmetry Prior](https://arxiv.org/pdf/2211.16927.pdf)

Fei Yin, Yong Zhang, **Xuan Wang**, Tengfei Wang, Xiaoyu Li, Yuan Gong, Yanbo Fan, Xiaodong Cun, Ying Shan, Cengiz Oztireli, Yujiu Yang

[**Project**](https://feiiyin.github.io/SPI/)  | [![](https://img.shields.io/github/stars/FeiiYin/SPI?style=flat-square&label=GitHub%20Star)](https://github.com/FeiiYin/SPI/)
<strong><span class='show_paper_citations' data=''></span></strong>
- We propose a novel method to promote 3D GAN inversion by introducing facial symmetry prior.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/LIRF.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Local Implicit Ray Function for Generalizable Radiance Field Representation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Local_Implicit_Ray_Function_for_Generalizable_Radiance_Field_Representation_CVPR_2023_paper.pdf)

Xin Huang, Qi Zhang, Ying Feng, Xiaoyu Li, **Xuan Wang**, Qing Wang

[**Project**](https://xhuangcv.github.io/lirf/) | [![](https://img.shields.io/github/stars/xhuangcv/lirf?style=flat-square&label=GitHub%20Star)](https://github.com/xhuangcv/lirf)
<strong><span class='show_paper_citations' data=''></span></strong>
- For generalisable neural radiance fileds, we propose LIRF to aggregate the information from conical frustums to construct a ray. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2023</div><img src='images/pub/HFCA.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[High-Fidelity Clothed Avatar Reconstruction from a Single Image](https://xuanwangvc.github.io/)

Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi, Xudong Liu, Guo-Jun Qi, Yong Zhang, **Xuan Wang**, Xiangyu Zhu, Zhen Lei

[**Project**](https://tingtingliao.github.io/CAR/) <strong><span class='show_paper_citations' data=''></span></strong>
- By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-tofine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image.
</div>
</div>
