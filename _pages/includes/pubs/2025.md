## Papers in 2025

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2025</div><img src='images/pub/avatarartist.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[AvatarArtist: Open-Domain 4D Avatarization](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_AvatarArtist_Open-Domain_4D_Avatarization_CVPR_2025_paper.pdf)

Hongyu Liu, **Xuan Wang**$^\dagger$, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, Qifeng Chen$^\dagger$

[**Project**](https://kumapowerliu.github.io/AvatarArtist/) | [![](https://img.shields.io/github/stars/ant-research/AvatarArtist?style=flat-square&label=GitHub%20Star)](https://github.com/ant-research/AvatarArtist)<strong><span class='show_paper_citations' data=''></span></strong>
- This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. Extensive experiments suggest that our model, termed AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2025</div><img src='images/pub/hera.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[HERA: Hybrid Explicit Representation for Ultra-Realistic Head Avatars](https://arxiv.org/pdf/2403.11453)

Hongrui Cai$^\star$, Yuting Xiao$^\star$, **Xuan Wang**$^\dagger$, Jiafei Li, Yudong Guo, Yanbo Fan, Shenghua Gao, Juyong Zhang$^\dagger$

 <strong><span class='show_paper_citations' data=''></span></strong>
- We present a hybrid explicit representation to combine the strengths of different geometric primitives, which adaptively models rich texture on smooth surfaces as well as complex geometric structures simultaneously.
- To avoid artifacts created by facet-crossing Gaussian splats, we design a stable depth sorting strategy based on the rasterization results of the mesh and 3DGS.
- We incorporate the proposed hybrid explicit representation into modeling 3D head avatars, which render more fidelity images in real time.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2025</div><img src='images/pub/tga.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations](https://xuanwangvc.github.io/)

Yating Wang, **Xuan Wang**, Ran Yi, Yanbo Fan, Jichen Hu, Jingcheng Zhu, Lizhuang Ma

 <strong><span class='show_paper_citations' data=''></span></strong>
- we propose a novel 3D head avatar modeling method that takes into account both dy064 namic texture modeling and spatiotemporal efficiency.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2025</div><img src='images/pub/dualtalk.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations](https://xuanwangvc.github.io/)

Ziqiao Peng, Yanbo Fan, Haoyu Wu, **Xuan Wang**, Hongyan Liu, Jun He, Zhaoxin Fan

 <strong><span class='show_paper_citations' data=''></span></strong>
- we introduce DualTalk, a novel unified framework that integrates the dy012 namic behaviors of speakers and listeners to simulate realistic and coherent dialogue interactions.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">CVPR 2025 (Highlight)</div><img src='images/pub/difflistener.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Diffusion-based Realistic Listening Head Generation via Hybrid Motion Modeling](https://xuanwangvc.github.io/)

Yinuo Wang, Yanbo Fan, **Xuan Wang**, Yu Guo, Fei Wang

 <strong><span class='show_paper_citations' data=''></span></strong>
- In this work, we propose a novel listening head generation framework that enables both highly expressive head motions and photorealistic rendering.
</div>
</div>
